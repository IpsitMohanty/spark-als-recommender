{
 "cells": [
  {
   "attachments": {
    "ibm-cloud.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ibm-cloud.png](attachment:ibm-cloud.png)\n",
    "\n",
    "## CASE STUDY - Deploying a recommender\n",
    "\n",
    "For this lab we will be using the MovieLens data :\n",
    "\n",
    "* [MovieLens Downloads](https://grouplens.org/datasets/movielens/latest/)\n",
    "\n",
    "download either **ml-latest-small.zip** or **ml-latest.zip** from this link and add the unziped folder to the data folder of the lab directory. We recommend you to use the small version if you are not working with a Spark cluster or a High memory machine.\n",
    "\n",
    "The two important pages for documentation are below.\n",
    "\n",
    "* [Spark MLlib collaborative filtering docs](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) \n",
    "* [Spark ALS docs](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark as ps\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\", \"data\")\n",
    "SAVE_DIR = os.path.join(\"..\", \"saved-recommender\")\n",
    "\n",
    "if os.path.isdir(SAVE_DIR):\n",
    "    shutil.rmtree(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "## ensure the spark context is available\n",
    "spark = (ps.sql.SparkSession.builder\n",
    "        .appName(\"sandbox\")\n",
    "        .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "print(spark.version) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure the data are downloaded, unziped and placed in the data folder of this lab.\n",
    "\n",
    "The data can be downloaded <a href=\"https://grouplens.org/datasets/movielens/\">here</a>. We recommend you to download the small version: <b>ml-latest-small.zip</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = os.path.join(\"work\", \"ratings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_file = os.path.join(\"/home/jovyan/work\", \"ratings.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|      1|       1|   4.0|964982703|\n",
      "|      1|       3|   4.0|964981247|\n",
      "|      1|       6|   4.0|964982224|\n",
      "|      1|      47|   5.0|964983815|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(ratings_file)\n",
    "df = df.withColumnRenamed(\"movieID\", \"movie_id\")\n",
    "df = df.withColumnRenamed(\"userID\", \"user_id\")\n",
    "df.show(n=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+---------+\n",
      "|user_id|movie_id|rating|timestamp|\n",
      "+-------+--------+------+---------+\n",
      "|      1|       1|   4.0|964982703|\n",
      "|      1|       3|   4.0|964981247|\n",
      "|      1|       6|   4.0|964982224|\n",
      "|      1|      47|   5.0|964983815|\n",
      "+-------+--------+------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the correct path to the ratings data file\n",
    "ratings_file = os.path.join(\"/home/jovyan/work\", \"ratings.csv\")  # Absolute path inside Docker container\n",
    "\n",
    "# Load the ratings data as a PySpark DataFrame\n",
    "df = spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(ratings_file)\n",
    "\n",
    "# Renaming the 'movieID' and 'userID' columns for consistency\n",
    "df = df.withColumnRenamed(\"movieID\", \"movie_id\")\n",
    "df = df.withColumnRenamed(\"userID\", \"user_id\")\n",
    "\n",
    "# Show the first 4 rows of the DataFrame\n",
    "df.show(n=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|movie_id|               title|              genres|\n",
      "+--------+--------------------+--------------------+\n",
      "|       1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|       2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|       3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|       4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the correct path to the movies data file\n",
    "movies_file = os.path.join(\"/home/jovyan/work\", \"movies.csv\")  # Absolute path inside Docker container\n",
    "\n",
    "# Load the movies data as a PySpark DataFrame\n",
    "movies_df = spark.read.format(\"csv\").options(header=\"true\", inferSchema=\"true\").load(movies_file)\n",
    "\n",
    "# Renaming the 'movieID' column to 'movie_id' for consistency\n",
    "movies_df = movies_df.withColumnRenamed(\"movieID\", \"movie_id\")\n",
    "\n",
    "# Show the first 4 rows of the DataFrame\n",
    "movies_df.show(n=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 1\n",
    "\n",
    "Explore the movie lens data a little and summarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "|summary|           user_id|        movie_id|            rating|           timestamp|\n",
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "|  count|            100836|          100836|            100836|              100836|\n",
      "|   mean|326.12756356856676|19435.2957177992| 3.501556983616962|1.2059460873684695E9|\n",
      "| stddev| 182.6184914635004|35530.9871987003|1.0425292390606342|2.1626103599513078E8|\n",
      "|    min|                 1|               1|               0.5|           828124615|\n",
      "|    max|               610|          193609|               5.0|          1537799250|\n",
      "+-------+------------------+----------------+------------------+--------------------+\n",
      "\n",
      "Unique users: 610\n",
      "Unique movies: 9724\n",
      "Movies with Rating > 2: 8852\n",
      "Movies with Rating > 3: 7363\n",
      "Movies with Rating > 4: 4056\n"
     ]
    }
   ],
   "source": [
    "# Show the summary statistics for the ratings data\n",
    "ratings_df.describe().show()\n",
    "\n",
    "# Count the number of unique users and movies\n",
    "unique_users = ratings_df.select(\"user_id\").distinct().count()\n",
    "unique_movies = ratings_df.select(\"movie_id\").distinct().count()\n",
    "\n",
    "# Count movies with rating > 2, > 3, and > 4\n",
    "movies_rating_gt_2 = ratings_df.filter(ratings_df.rating > 2).select(\"movie_id\").distinct().count()\n",
    "movies_rating_gt_3 = ratings_df.filter(ratings_df.rating > 3).select(\"movie_id\").distinct().count()\n",
    "movies_rating_gt_4 = ratings_df.filter(ratings_df.rating > 4).select(\"movie_id\").distinct().count()\n",
    "\n",
    "# Display the results\n",
    "print(f\"Unique users: {unique_users}\")\n",
    "print(f\"Unique movies: {unique_movies}\")\n",
    "print(f\"Movies with Rating > 2: {movies_rating_gt_2}\")\n",
    "print(f\"Movies with Rating > 3: {movies_rating_gt_3}\")\n",
    "print(f\"Movies with Rating > 4: {movies_rating_gt_4}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 2\n",
    "\n",
    "Find the ten most popular movies. \n",
    "\n",
    "\n",
    "1. Create 2 pyspark dataframes one with the count of each film in df and one with the average rating of each movie in df.\n",
    "2. Join these two dataframes in a third dataframe. Then, filter this dataframe to select only the movies that have been seen more than 100 times.\n",
    "3. Use the movies_df dataframe to add the names of each movies on the dataframe created in 2. Then, order the dataframe by descending average rating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+-----+--------------------+--------------------+\n",
      "|movie_id|      avg(rating)|count|               title|              genres|\n",
      "+--------+-----------------+-----+--------------------+--------------------+\n",
      "|     318|4.429022082018927|  317|Shawshank Redempt...|         Crime|Drama|\n",
      "|     858|        4.2890625|  192|Godfather, The (1...|         Crime|Drama|\n",
      "|    2959|4.272935779816514|  218|   Fight Club (1999)|Action|Crime|Dram...|\n",
      "|    1221| 4.25968992248062|  129|Godfather: Part I...|         Crime|Drama|\n",
      "|   48516|4.252336448598131|  107|Departed, The (2006)|Crime|Drama|Thriller|\n",
      "|    1213|             4.25|  126|   Goodfellas (1990)|         Crime|Drama|\n",
      "|   58559|4.238255033557047|  149|Dark Knight, The ...|Action|Crime|Dram...|\n",
      "|      50|4.237745098039215|  204|Usual Suspects, T...|Crime|Mystery|Thr...|\n",
      "|    1197|4.232394366197183|  142|Princess Bride, T...|Action|Adventure|...|\n",
      "|     260|4.231075697211155|  251|Star Wars: Episod...|Action|Adventure|...|\n",
      "+--------+-----------------+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1_ Count the number of ratings for each movie\n",
    "movie_counts = ratings_df.groupBy(\"movie_id\").count()\n",
    "\n",
    "# Calculate the average rating for each movie\n",
    "top_rated = ratings_df.groupBy(\"movie_id\").avg(\"rating\")\n",
    "\n",
    "# 2_ Join the two DataFrames (movie_counts and top_rated)\n",
    "top_movies = movie_counts.join(top_rated, on=\"movie_id\", how=\"inner\")\n",
    "\n",
    "# 3_ Filter out movies that have been rated fewer than 100 times\n",
    "top_movies = top_movies.filter(top_movies[\"count\"] > 100)\n",
    "\n",
    "# Join with the movies DataFrame to get movie names and genres\n",
    "top_movies = top_movies.join(movies_df, on=\"movie_id\", how=\"inner\")\n",
    "\n",
    "# 4_ Order the movies by descending average rating\n",
    "top_movies = top_movies.orderBy(top_movies[\"avg(rating)\"], ascending=False)\n",
    "\n",
    "# Select the relevant columns (movie_id, avg(rating), count, title, genres)\n",
    "top_movies = top_movies.select(\"movie_id\", \"avg(rating)\", \"count\", \"title\", \"genres\")\n",
    "\n",
    "# Show the top 10 most popular movies\n",
    "top_movies.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 3\n",
    "\n",
    "We will now fit a ALS model, this is matrix factorization model used for rating recommendation. See the [Spark ALS docs](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS)\n",
    "for example usage. \n",
    "\n",
    "First we split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function called **train_model()** that takes two inputs :\n",
    "\n",
    "1. ``reg_param`` : the regularization parameter of the factorization model\n",
    "2. ``implicit_prefs`` : a boolean variable that indicate whereas the model should used explicit or implicit ratings.\n",
    "    \n",
    "The function train an ALS model on the training set then predict the test set and evaluate this prediction.\n",
    "The output of the function should be the RMSE of the fitted model on the test set./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "import numpy as np\n",
    "\n",
    "def train_model(reg_param, implicit_prefs=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate an ALS model\n",
    "    Inputs:\n",
    "        reg_param (float): The regularization parameter of the ALS model.\n",
    "        implicit_prefs (bool): A flag indicating whether the model should use implicit ratings.\n",
    "        \n",
    "    Outputs:\n",
    "        RMSE (float): The Root Mean Squared Error (RMSE) of the fitted model on the test set.\n",
    "    \"\"\"\n",
    "    # Initialize the ALS model\n",
    "    als = ALS(userCol=\"user_id\", itemCol=\"movie_id\", ratingCol=\"rating\", \n",
    "              regParam=reg_param, implicitPrefs=implicit_prefs, coldStartStrategy=\"drop\")\n",
    "    \n",
    "    # Fit the model on the training data\n",
    "    model = als.fit(training)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    predictions = model.transform(test)\n",
    "    \n",
    "    # Initialize the evaluator to calculate RMSE\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "    \n",
    "    # Evaluate the RMSE of the model on the test set\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    # Print the result\n",
    "    print(f\"regParam={reg_param}, RMSE={np.round(rmse, 2)}\")\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the function created above for several ``reg_param`` values find the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regParam=0.01, RMSE=1.12\n",
      "regParam=0.05, RMSE=0.94\n",
      "regParam=0.1, RMSE=0.88\n",
      "regParam=0.15, RMSE=0.87\n",
      "regParam=0.25, RMSE=0.9\n"
     ]
    }
   ],
   "source": [
    "for reg_param in [0.01, 0.05, 0.1, 0.15, 0.25]:\n",
    "    train_model(reg_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 4\n",
    "\n",
    "With your best regParam try using the `implicitPrefs` flag.\n",
    "\n",
    ">Note that the results here make sense because the data are `explicit` ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with reg_param=0.05 and implicitPrefs=True\n",
      "regParam=0.05, RMSE=3.23\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2255577890312352"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming the best regularization parameter from Question 3 is 0.05 (replace with the best one you got)\n",
    "best_reg_param = 0.05  # Replace with your best reg_param value\n",
    "\n",
    "# Train and evaluate the ALS model with implicitPrefs set to True\n",
    "print(f\"Training with reg_param={best_reg_param} and implicitPrefs=True\")\n",
    "train_model(best_reg_param, implicit_prefs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 5\n",
    "\n",
    "Retrain the model with your best ``reg_param`` and ``implicit_prefs`` on the entire dataset and save the trained model in the SAVE_DIR directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training\n",
      "...saving ALS model\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Assuming the best regularization parameter from Question 3 is 0.05 (replace with the best one you got)\n",
    "best_reg_param = 0.05  # Replace with the best reg_param obtained earlier\n",
    "\n",
    "# Re-train the model using the entire dataset\n",
    "print(\"...training\")\n",
    "als = ALS(userCol=\"user_id\", itemCol=\"movie_id\", ratingCol=\"rating\", \n",
    "          regParam=best_reg_param, implicitPrefs=True, coldStartStrategy=\"drop\")\n",
    "model = als.fit(ratings_df)  # Use the entire ratings data (ratings_df)\n",
    "\n",
    "# Save the trained ALS model in the SAVE_DIR\n",
    "print(\"...saving ALS model\")\n",
    "model.save(SAVE_DIR)  # Saving the model to the specified directory\n",
    "\n",
    "print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION 6\n",
    "\n",
    "We now want to use ``spark-submit`` to load the model and demonstrate that you can load the model and interface with it.\n",
    "\n",
    "Following the best practices we created a python script (``recommender-submit.py``) in the **scripts** folder that loads the model, creates some hand crafted data points and query the model. We recommend you to go over this script and make sure you understand it before running it through this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/15 08:04:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/15 08:04:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "best rated [(260,), (2628,), (1196,), (122886,), (187595,), (179819,), (1210,)]\n",
      "25/02/15 08:04:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "closest_users                                                                   \n",
      " [(596,), (561,), (599,), (380,), (292,), (21,), (586,), (534,), (18,), (305,), (483,), (68,), (573,), (448,), (328,), (352,), (414,), (63,), (514,), (160,), (288,), (91,), (249,), (603,), (62,), (555,), (19,), (220,), (590,), (57,), (45,), (313,), (217,), (330,), (425,), (135,), (202,), (332,), (239,), (520,), (580,), (570,), (489,), (434,), (452,), (182,), (368,), (391,), (354,), (453,), (475,), (469,), (186,), (66,), (608,), (279,), (376,), (597,), (477,), (219,), (198,), (1,), (15,), (304,), (247,), (522,), (166,), (64,), (282,), (428,), (42,), (266,), (607,), (167,), (212,), (129,), (141,), (263,), (577,), (560,), (606,), (551,), (464,), (527,), (432,), (312,), (122,), (201,), (286,), (601,), (195,), (325,), (233,), (474,), (103,), (610,), (525,), (567,), (438,), (144,), (357,), (339,), (125,), (230,), (17,), (381,), (294,), (234,), (200,), (265,), (177,), (226,), (82,), (199,), (552,), (275,), (95,), (562,), (210,), (119,), (274,), (184,), (509,), (318,), (382,), (334,), (298,), (365,), (73,), (256,), (408,), (50,), (306,), (98,), (480,), (246,), (582,), (139,), (372,), (140,), (593,), (301,), (52,), (385,), (466,), (524,), (111,), (137,), (248,), (154,), (393,), (115,), (148,), (600,), (51,), (511,), (28,), (523,), (401,), (252,), (7,), (387,), (331,), (594,), (291,), (30,), (123,), (550,), (106,), (495,), (517,), (105,), (211,), (10,), (227,), (24,), (319,), (344,), (80,), (462,), (153,), (418,), (417,), (222,), (308,), (490,), (564,), (41,), (317,), (326,), (378,), (605,), (492,), (104,), (515,), (556,), (25,), (272,), (114,), (459,), (6,)]\n"
     ]
    }
   ],
   "source": [
    "!python recommender-submit.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/15 08:06:00 INFO SparkContext: Running Spark version 3.5.0\n",
      "25/02/15 08:06:00 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/02/15 08:06:00 INFO SparkContext: Java version 17.0.8.1\n",
      "25/02/15 08:06:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/15 08:06:00 INFO ResourceUtils: ==============================================================\n",
      "25/02/15 08:06:00 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "25/02/15 08:06:00 INFO ResourceUtils: ==============================================================\n",
      "25/02/15 08:06:00 INFO SparkContext: Submitted application: recommend\n",
      "25/02/15 08:06:00 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/02/15 08:06:00 INFO ResourceProfile: Limiting resource is cpu\n",
      "25/02/15 08:06:00 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "25/02/15 08:06:01 INFO SecurityManager: Changing view acls to: jovyan\n",
      "25/02/15 08:06:01 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "25/02/15 08:06:01 INFO SecurityManager: Changing view acls groups to: \n",
      "25/02/15 08:06:01 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/02/15 08:06:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: jovyan; groups with view permissions: EMPTY; users with modify permissions: jovyan; groups with modify permissions: EMPTY\n",
      "25/02/15 08:06:01 INFO Utils: Successfully started service 'sparkDriver' on port 43077.\n",
      "25/02/15 08:06:01 INFO SparkEnv: Registering MapOutputTracker\n",
      "25/02/15 08:06:01 INFO SparkEnv: Registering BlockManagerMaster\n",
      "25/02/15 08:06:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/02/15 08:06:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "25/02/15 08:06:01 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "25/02/15 08:06:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c0c6b997-2057-4fc1-8bad-b35212e03518\n",
      "25/02/15 08:06:01 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "25/02/15 08:06:01 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "25/02/15 08:06:01 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/02/15 08:06:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/02/15 08:06:01 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "25/02/15 08:06:02 INFO Executor: Starting executor ID driver on host 36ea6075e503\n",
      "25/02/15 08:06:02 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/02/15 08:06:02 INFO Executor: Java version 17.0.8.1\n",
      "25/02/15 08:06:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "25/02/15 08:06:02 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5f6c33ba for default.\n",
      "25/02/15 08:06:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40667.\n",
      "25/02/15 08:06:02 INFO NettyBlockTransferService: Server created on 36ea6075e503:40667\n",
      "25/02/15 08:06:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/02/15 08:06:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 36ea6075e503, 40667, None)\n",
      "25/02/15 08:06:02 INFO BlockManagerMasterEndpoint: Registering block manager 36ea6075e503:40667 with 434.4 MiB RAM, BlockManagerId(driver, 36ea6075e503, 40667, None)\n",
      "25/02/15 08:06:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 36ea6075e503, 40667, None)\n",
      "25/02/15 08:06:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 36ea6075e503, 40667, None)\n",
      "25/02/15 08:06:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 221.5 KiB, free 434.2 MiB)\n",
      "25/02/15 08:06:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 434.2 MiB)\n",
      "25/02/15 08:06:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 36ea6075e503:40667 (size: 32.6 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:03 INFO SparkContext: Created broadcast 0 from textFile at ReadWrite.scala:587\n",
      "25/02/15 08:06:03 INFO FileInputFormat: Total input files to process : 1\n",
      "25/02/15 08:06:03 INFO SparkContext: Starting job: first at ReadWrite.scala:587\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Got job 0 (first at ReadWrite.scala:587) with 1 output partitions\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Final stage: ResultStage 0 (first at ReadWrite.scala:587)\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Submitting ResultStage 0 (../saved-recommender/metadata MapPartitionsRDD[1] at textFile at ReadWrite.scala:587), which has no missing parents\n",
      "25/02/15 08:06:03 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 4.9 KiB, free 434.1 MiB)\n",
      "25/02/15 08:06:03 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.8 KiB, free 434.1 MiB)\n",
      "25/02/15 08:06:03 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 36ea6075e503:40667 (size: 2.8 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:03 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (../saved-recommender/metadata MapPartitionsRDD[1] at textFile at ReadWrite.scala:587) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/15 08:06:03 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/02/15 08:06:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (36ea6075e503, executor driver, partition 0, PROCESS_LOCAL, 7686 bytes) \n",
      "25/02/15 08:06:03 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/02/15 08:06:03 INFO HadoopRDD: Input split: file:/home/jovyan/saved-recommender/metadata/part-00000:0+364\n",
      "25/02/15 08:06:03 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1347 bytes result sent to driver\n",
      "25/02/15 08:06:03 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 271 ms on 36ea6075e503 (executor driver) (1/1)\n",
      "25/02/15 08:06:03 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:03 INFO DAGScheduler: ResultStage 0 (first at ReadWrite.scala:587) finished in 0.484 s\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/15 08:06:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/02/15 08:06:03 INFO DAGScheduler: Job 0 finished: first at ReadWrite.scala:587, took 0.561486 s\n",
      "25/02/15 08:06:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/02/15 08:06:04 INFO SharedState: Warehouse path is 'file:/home/jovyan/work/spark-warehouse'.\n",
      "25/02/15 08:06:04 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 36ea6075e503:40667 in memory (size: 2.8 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:05 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 36ea6075e503:40667 in memory (size: 32.6 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:06 INFO InMemoryFileIndex: It took 26 ms to list leaf files for 1 paths.\n",
      "25/02/15 08:06:06 INFO SparkContext: Starting job: load at ALS.scala:567\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Got job 1 (load at ALS.scala:567) with 1 output partitions\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Final stage: ResultStage 1 (load at ALS.scala:567)\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at load at ALS.scala:567), which has no missing parents\n",
      "25/02/15 08:06:06 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 103.1 KiB, free 434.3 MiB)\n",
      "25/02/15 08:06:06 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
      "25/02/15 08:06:06 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 36ea6075e503:40667 (size: 37.2 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:06 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at load at ALS.scala:567) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/15 08:06:06 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/02/15 08:06:06 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (36ea6075e503, executor driver, partition 0, PROCESS_LOCAL, 7825 bytes) \n",
      "25/02/15 08:06:06 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/02/15 08:06:06 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1880 bytes result sent to driver\n",
      "25/02/15 08:06:06 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 355 ms on 36ea6075e503 (executor driver) (1/1)\n",
      "25/02/15 08:06:06 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:06 INFO DAGScheduler: ResultStage 1 (load at ALS.scala:567) finished in 0.405 s\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/15 08:06:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/02/15 08:06:06 INFO DAGScheduler: Job 1 finished: load at ALS.scala:567, took 0.418004 s\n",
      "25/02/15 08:06:07 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 36ea6075e503:40667 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:07 INFO InMemoryFileIndex: It took 7 ms to list leaf files for 1 paths.\n",
      "25/02/15 08:06:07 INFO SparkContext: Starting job: load at ALS.scala:569\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Got job 2 (load at ALS.scala:569) with 1 output partitions\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Final stage: ResultStage 2 (load at ALS.scala:569)\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at load at ALS.scala:569), which has no missing parents\n",
      "25/02/15 08:06:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 103.1 KiB, free 434.3 MiB)\n",
      "25/02/15 08:06:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 37.2 KiB, free 434.3 MiB)\n",
      "25/02/15 08:06:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 36ea6075e503:40667 (size: 37.2 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at load at ALS.scala:569) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/15 08:06:07 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/02/15 08:06:07 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (36ea6075e503, executor driver, partition 0, PROCESS_LOCAL, 7825 bytes) \n",
      "25/02/15 08:06:07 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "25/02/15 08:06:07 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1794 bytes result sent to driver\n",
      "25/02/15 08:06:07 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 37 ms on 36ea6075e503 (executor driver) (1/1)\n",
      "25/02/15 08:06:07 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:07 INFO DAGScheduler: ResultStage 2 (load at ALS.scala:569) finished in 0.082 s\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/15 08:06:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/02/15 08:06:07 INFO DAGScheduler: Job 2 finished: load at ALS.scala:569, took 0.092693 s\n",
      "25/02/15 08:06:08 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 36ea6075e503:40667 in memory (size: 37.2 KiB, free: 434.4 MiB)\n",
      "best rated [(260,), (2628,), (1196,), (122886,), (187595,), (179819,), (1210,)]\n",
      "25/02/15 08:06:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(id)\n",
      "25/02/15 08:06:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(id#4)\n",
      "25/02/15 08:06:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/02/15 08:06:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/02/15 08:06:11 INFO CodeGenerator: Code generated in 278.043619 ms\n",
      "25/02/15 08:06:11 INFO CodeGenerator: Code generated in 282.646708 ms\n",
      "25/02/15 08:06:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 201.0 KiB, free 434.2 MiB)\n",
      "25/02/15 08:06:11 INFO CodeGenerator: Code generated in 40.975705 ms\n",
      "25/02/15 08:06:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 201.0 KiB, free 434.0 MiB)\n",
      "25/02/15 08:06:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 434.0 MiB)\n",
      "25/02/15 08:06:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 36ea6075e503:40667 (size: 35.0 KiB, free: 434.4 MiB)\n",
      "25/02/15 08:06:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 35.0 KiB, free 433.9 MiB)\n",
      "25/02/15 08:06:11 INFO SparkContext: Created broadcast 4 from toPandas at /home/jovyan/work/recommender-submit.py:37\n",
      "25/02/15 08:06:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 36ea6075e503:40667 (size: 35.0 KiB, free: 434.3 MiB)\n",
      "25/02/15 08:06:11 INFO SparkContext: Created broadcast 5 from toPandas at /home/jovyan/work/recommender-submit.py:37\n",
      "25/02/15 08:06:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10494496 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/15 08:06:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 10594983 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/02/15 08:06:11 INFO DAGScheduler: Registering RDD 18 (toPandas at /home/jovyan/work/recommender-submit.py:37) as input to shuffle 0\n",
      "25/02/15 08:06:11 INFO DAGScheduler: Got map stage job 3 (toPandas at /home/jovyan/work/recommender-submit.py:37) with 4 output partitions\n",
      "25/02/15 08:06:11 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (toPandas at /home/jovyan/work/recommender-submit.py:37)\n",
      "25/02/15 08:06:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/15 08:06:11 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:11 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[18] at toPandas at /home/jovyan/work/recommender-submit.py:37), which has no missing parents\n",
      "25/02/15 08:06:11 INFO CodeGenerator: Code generated in 28.351662 ms\n",
      "25/02/15 08:06:12 INFO SparkContext: Starting job: toPandas at /home/jovyan/work/recommender-submit.py:37\n",
      "25/02/15 08:06:12 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 27.2 KiB, free 433.9 MiB)\n",
      "25/02/15 08:06:12 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 12.1 KiB, free 433.9 MiB)\n",
      "25/02/15 08:06:12 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 36ea6075e503:40667 (size: 12.1 KiB, free: 434.3 MiB)\n",
      "25/02/15 08:06:12 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[18] at toPandas at /home/jovyan/work/recommender-submit.py:37) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "25/02/15 08:06:12 INFO TaskSchedulerImpl: Adding task set 3.0 with 4 tasks resource profile 0\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Registering RDD 23 (toPandas at /home/jovyan/work/recommender-submit.py:37) as input to shuffle 1\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Got map stage job 4 (toPandas at /home/jovyan/work/recommender-submit.py:37) with 4 output partitions\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (toPandas at /home/jovyan/work/recommender-submit.py:37)\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:12 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (36ea6075e503, executor driver, partition 0, PROCESS_LOCAL, 8632 bytes) \n",
      "25/02/15 08:06:12 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4) (36ea6075e503, executor driver, partition 1, PROCESS_LOCAL, 8632 bytes) \n",
      "25/02/15 08:06:12 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 5) (36ea6075e503, executor driver, partition 2, PROCESS_LOCAL, 8632 bytes) \n",
      "25/02/15 08:06:12 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 6) (36ea6075e503, executor driver, partition 3, PROCESS_LOCAL, 8284 bytes) \n",
      "25/02/15 08:06:12 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[23] at toPandas at /home/jovyan/work/recommender-submit.py:37), which has no missing parents\n",
      "25/02/15 08:06:12 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)\n",
      "25/02/15 08:06:12 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 15.0 KiB, free 433.9 MiB)\n",
      "25/02/15 08:06:12 INFO Executor: Running task 2.0 in stage 3.0 (TID 5)\n",
      "25/02/15 08:06:12 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 433.9 MiB)\n",
      "25/02/15 08:06:12 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 36ea6075e503:40667 (size: 7.9 KiB, free: 434.3 MiB)\n",
      "25/02/15 08:06:12 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[23] at toPandas at /home/jovyan/work/recommender-submit.py:37) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "25/02/15 08:06:12 INFO TaskSchedulerImpl: Adding task set 4.0 with 4 tasks resource profile 0\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Got job 5 (toPandas at /home/jovyan/work/recommender-submit.py:37) with 4 output partitions\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Final stage: ResultStage 5 (toPandas at /home/jovyan/work/recommender-submit.py:37)\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[21] at toPandas at /home/jovyan/work/recommender-submit.py:37), which has no missing parents\n",
      "25/02/15 08:06:12 INFO Executor: Running task 3.0 in stage 3.0 (TID 6)\n",
      "25/02/15 08:06:12 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 38.6 KiB, free 433.8 MiB)\n",
      "25/02/15 08:06:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.1 KiB, free 433.8 MiB)\n",
      "25/02/15 08:06:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 36ea6075e503:40667 (size: 16.1 KiB, free: 434.3 MiB)\n",
      "25/02/15 08:06:12 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:12 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 5 (MapPartitionsRDD[21] at toPandas at /home/jovyan/work/recommender-submit.py:37) (first 15 tasks are for partitions Vector(0, 1, 2, 3))\n",
      "25/02/15 08:06:12 INFO TaskSchedulerImpl: Adding task set 5.0 with 4 tasks resource profile 0\n",
      "25/02/15 08:06:12 INFO CodeGenerator: Code generated in 51.182557 ms\n",
      "25/02/15 08:06:12 INFO CodeGenerator: Code generated in 23.060324 ms\n",
      "25/02/15 08:06:12 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00002-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-44156, partition values: [empty row]\n",
      "25/02/15 08:06:12 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00004-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-44905, partition values: [empty row]\n",
      "25/02/15 08:06:12 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00001-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-43319, partition values: [empty row]\n",
      "25/02/15 08:06:12 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00009-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-42700, partition values: [empty row]\n",
      "25/02/15 08:06:12 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:12 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:12 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:12 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:12 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "25/02/15 08:06:12 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "25/02/15 08:06:12 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "25/02/15 08:06:12 INFO CodecPool: Got brand-new decompressor [.snappy]\n",
      "25/02/15 08:06:12 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00000-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-44070, partition values: [empty row]\n",
      "25/02/15 08:06:13 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:13 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00008-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-42967, partition values: [empty row]\n",
      "25/02/15 08:06:13 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00003-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-43542, partition values: [empty row]\n",
      "25/02/15 08:06:13 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00005-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-44332, partition values: [empty row]\n",
      "25/02/15 08:06:13 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:13 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:13 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:13 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00006-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-42745, partition values: [empty row]\n",
      "25/02/15 08:06:13 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:13 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/itemFactors/part-00007-0170903f-13a5-4e3f-a5db-7079112f349c-c000.snappy.parquet, range: 0-44156, partition values: [empty row]\n",
      "25/02/15 08:06:13 INFO FilterCompat: Filtering using predicate: noteq(id, null)\n",
      "25/02/15 08:06:13 INFO Executor: Finished task 3.0 in stage 3.0 (TID 6). 2339 bytes result sent to driver\n",
      "25/02/15 08:06:13 INFO Executor: Finished task 2.0 in stage 3.0 (TID 5). 2296 bytes result sent to driver\n",
      "25/02/15 08:06:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 7) (36ea6075e503, executor driver, partition 0, PROCESS_LOCAL, 7614 bytes) \n",
      "25/02/15 08:06:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 7)\n",
      "25/02/15 08:06:13 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 8) (36ea6075e503, executor driver, partition 1, PROCESS_LOCAL, 7644 bytes) \n",
      "25/02/15 08:06:13 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 2296 bytes result sent to driver\n",
      "25/02/15 08:06:13 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 9) (36ea6075e503, executor driver, partition 2, PROCESS_LOCAL, 7648 bytes) \n",
      "25/02/15 08:06:13 INFO Executor: Running task 2.0 in stage 4.0 (TID 9)\n",
      "25/02/15 08:06:13 INFO Executor: Running task 1.0 in stage 4.0 (TID 8)\n",
      "25/02/15 08:06:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 2296 bytes result sent to driver\n",
      "25/02/15 08:06:13 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 1157 ms on 36ea6075e503 (executor driver) (1/4)\n",
      "25/02/15 08:06:13 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 5) in 1156 ms on 36ea6075e503 (executor driver) (2/4)\n",
      "25/02/15 08:06:13 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 6) in 1159 ms on 36ea6075e503 (executor driver) (3/4)\n",
      "25/02/15 08:06:13 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 10) (36ea6075e503, executor driver, partition 3, PROCESS_LOCAL, 7646 bytes) \n",
      "25/02/15 08:06:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1172 ms on 36ea6075e503 (executor driver) (4/4)\n",
      "25/02/15 08:06:13 INFO Executor: Running task 3.0 in stage 4.0 (TID 10)\n",
      "25/02/15 08:06:13 INFO DAGScheduler: ShuffleMapStage 3 (toPandas at /home/jovyan/work/recommender-submit.py:37) finished in 1.243 s\n",
      "25/02/15 08:06:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/02/15 08:06:13 INFO DAGScheduler: running: Set(ResultStage 5, ShuffleMapStage 4)\n",
      "25/02/15 08:06:13 INFO DAGScheduler: waiting: Set()\n",
      "25/02/15 08:06:13 INFO DAGScheduler: failed: Set()\n",
      "25/02/15 08:06:14 INFO CodeGenerator: Code generated in 65.831441 ms\n",
      "25/02/15 08:06:14 INFO CodeGenerator: Code generated in 130.01882 ms\n",
      "25/02/15 08:06:14 INFO PythonRunner: Times: total = 1610, boot = 971, init = 639, finish = 0\n",
      "25/02/15 08:06:14 INFO PythonRunner: Times: total = 1664, boot = 980, init = 684, finish = 0\n",
      "25/02/15 08:06:15 INFO Executor: Finished task 3.0 in stage 4.0 (TID 10). 2436 bytes result sent to driver\n",
      "25/02/15 08:06:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11) (36ea6075e503, executor driver, partition 0, PROCESS_LOCAL, 8643 bytes) \n",
      "25/02/15 08:06:15 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 10) in 1887 ms on 36ea6075e503 (executor driver) (1/4)\n",
      "25/02/15 08:06:15 INFO Executor: Running task 0.0 in stage 5.0 (TID 11)\n",
      "25/02/15 08:06:15 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 36275\n",
      "25/02/15 08:06:15 INFO Executor: Finished task 1.0 in stage 4.0 (TID 8). 2436 bytes result sent to driver\n",
      "25/02/15 08:06:15 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 12) (36ea6075e503, executor driver, partition 1, PROCESS_LOCAL, 8643 bytes) \n",
      "25/02/15 08:06:15 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 8) in 1951 ms on 36ea6075e503 (executor driver) (2/4)\n",
      "25/02/15 08:06:15 INFO Executor: Running task 1.0 in stage 5.0 (TID 12)\n",
      "25/02/15 08:06:15 INFO PythonRunner: Times: total = 1809, boot = 917, init = 891, finish = 1\n",
      "25/02/15 08:06:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 7). 2479 bytes result sent to driver\n",
      "25/02/15 08:06:15 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 13) (36ea6075e503, executor driver, partition 2, PROCESS_LOCAL, 8643 bytes) \n",
      "25/02/15 08:06:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 7) in 2139 ms on 36ea6075e503 (executor driver) (3/4)\n",
      "25/02/15 08:06:15 INFO Executor: Running task 2.0 in stage 5.0 (TID 13)\n",
      "25/02/15 08:06:15 INFO PythonRunner: Times: total = 2005, boot = 941, init = 1064, finish = 0\n",
      "25/02/15 08:06:15 INFO Executor: Finished task 2.0 in stage 4.0 (TID 9). 2436 bytes result sent to driver\n",
      "25/02/15 08:06:15 INFO TaskSetManager: Starting task 3.0 in stage 5.0 (TID 14) (36ea6075e503, executor driver, partition 3, PROCESS_LOCAL, 8295 bytes) \n",
      "25/02/15 08:06:15 INFO Executor: Running task 3.0 in stage 5.0 (TID 14)\n",
      "25/02/15 08:06:15 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 9) in 2261 ms on 36ea6075e503 (executor driver) (4/4)\n",
      "25/02/15 08:06:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:15 INFO DAGScheduler: ShuffleMapStage 4 (toPandas at /home/jovyan/work/recommender-submit.py:37) finished in 3.413 s\n",
      "25/02/15 08:06:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/02/15 08:06:15 INFO DAGScheduler: running: Set(ResultStage 5)\n",
      "25/02/15 08:06:15 INFO DAGScheduler: waiting: Set()\n",
      "25/02/15 08:06:15 INFO DAGScheduler: failed: Set()\n",
      "25/02/15 08:06:15 INFO CodeGenerator: Code generated in 222.28419 ms\n",
      "25/02/15 08:06:15 INFO CodeGenerator: Code generated in 60.208521 ms\n",
      "25/02/15 08:06:15 INFO CodeGenerator: Code generated in 61.519605 ms\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00004-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3495, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00007-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3492, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00005-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3495, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00006-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3495, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00003-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3495, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00008-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3495, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00009-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3494, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00001-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3495, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00002-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3494, partition values: [empty row]\n",
      "25/02/15 08:06:15 INFO Executor: Finished task 3.0 in stage 5.0 (TID 14). 4830 bytes result sent to driver\n",
      "25/02/15 08:06:15 INFO TaskSetManager: Finished task 3.0 in stage 5.0 (TID 14) in 508 ms on 36ea6075e503 (executor driver) (1/4)\n",
      "25/02/15 08:06:15 INFO FileScanRDD: Reading File path: file:///home/jovyan/saved-recommender/userFactors/part-00000-7e16b8e3-e2b5-43f5-94b6-1e3b285adda3-c000.snappy.parquet, range: 0-3495, partition values: [empty row]\n",
      "25/02/15 08:06:16 INFO Executor: Finished task 2.0 in stage 5.0 (TID 13). 10320 bytes result sent to driver\n",
      "25/02/15 08:06:16 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 13) in 752 ms on 36ea6075e503 (executor driver) (2/4)\n",
      "25/02/15 08:06:16 INFO Executor: Finished task 1.0 in stage 5.0 (TID 12). 10272 bytes result sent to driver\n",
      "25/02/15 08:06:16 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 12) in 964 ms on 36ea6075e503 (executor driver) (3/4)\n",
      "25/02/15 08:06:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 11). 10278 bytes result sent to driver\n",
      "25/02/15 08:06:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 1021 ms on 36ea6075e503 (executor driver) (4/4)\n",
      "25/02/15 08:06:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:16 INFO DAGScheduler: ResultStage 5 (toPandas at /home/jovyan/work/recommender-submit.py:37) finished in 4.026 s\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/15 08:06:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Job 5 finished: toPandas at /home/jovyan/work/recommender-submit.py:37, took 4.138723 s\n",
      "25/02/15 08:06:16 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 27.5 KiB, free 433.8 MiB)\n",
      "25/02/15 08:06:16 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 26.7 KiB, free 433.8 MiB)\n",
      "25/02/15 08:06:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 36ea6075e503:40667 (size: 26.7 KiB, free: 434.3 MiB)\n",
      "25/02/15 08:06:16 INFO SparkContext: Created broadcast 9 from toPandas at /home/jovyan/work/recommender-submit.py:37\n",
      "25/02/15 08:06:16 INFO ShufflePartitionsUtil: For shuffle(0, 1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/02/15 08:06:16 INFO CodeGenerator: Code generated in 36.969241 ms\n",
      "25/02/15 08:06:16 INFO CodeGenerator: Code generated in 70.473634 ms\n",
      "25/02/15 08:06:16 INFO CodeGenerator: Code generated in 57.677034 ms\n",
      "25/02/15 08:06:16 INFO CodeGenerator: Code generated in 37.59783 ms\n",
      "25/02/15 08:06:16 INFO CodeGenerator: Code generated in 20.541617 ms\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Registering RDD 37 (toPandas at /home/jovyan/work/recommender-submit.py:37) as input to shuffle 2\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Got map stage job 6 (toPandas at /home/jovyan/work/recommender-submit.py:37) with 1 output partitions\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Final stage: ShuffleMapStage 8 (toPandas at /home/jovyan/work/recommender-submit.py:37)\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6, ShuffleMapStage 7)\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:16 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[37] at toPandas at /home/jovyan/work/recommender-submit.py:37), which has no missing parents\n",
      "25/02/15 08:06:17 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 90.7 KiB, free 433.7 MiB)\n",
      "25/02/15 08:06:17 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 38.1 KiB, free 433.6 MiB)\n",
      "25/02/15 08:06:17 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 36ea6075e503:40667 (size: 38.1 KiB, free: 434.2 MiB)\n",
      "25/02/15 08:06:17 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[37] at toPandas at /home/jovyan/work/recommender-submit.py:37) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/15 08:06:17 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "25/02/15 08:06:17 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15) (36ea6075e503, executor driver, partition 0, NODE_LOCAL, 7886 bytes) \n",
      "25/02/15 08:06:17 INFO Executor: Running task 0.0 in stage 8.0 (TID 15)\n",
      "25/02/15 08:06:17 INFO ShuffleBlockFetcherIterator: Getting 4 (552.0 KiB) non-empty blocks including 4 (552.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/15 08:06:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 15.26998 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 50.261081 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 16.788494 ms\n",
      "25/02/15 08:06:17 INFO ShuffleBlockFetcherIterator: Getting 4 (420.0 B) non-empty blocks including 4 (420.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/15 08:06:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 34.663381 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 7.272874 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 19.274366 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 41.393958 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 14.393884 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 37.613932 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 19.697015 ms\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 24.337134 ms\n",
      "25/02/15 08:06:17 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "25/02/15 08:06:17 INFO CodeGenerator: Code generated in 9.116949 ms\n",
      "25/02/15 08:06:18 INFO CodeGenerator: Code generated in 6.288852 ms\n",
      "25/02/15 08:06:18 INFO CodeGenerator: Code generated in 5.781782 ms\n",
      "25/02/15 08:06:18 INFO CodeGenerator: Code generated in 30.01277 ms\n",
      "25/02/15 08:06:18 INFO CodeGenerator: Code generated in 10.368432 ms\n",
      "25/02/15 08:06:18 INFO Executor: Finished task 0.0 in stage 8.0 (TID 15). 8618 bytes result sent to driver\n",
      "25/02/15 08:06:18 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 1087 ms on 36ea6075e503 (executor driver) (1/1)\n",
      "25/02/15 08:06:18 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:18 INFO DAGScheduler: ShuffleMapStage 8 (toPandas at /home/jovyan/work/recommender-submit.py:37) finished in 1.218 s\n",
      "25/02/15 08:06:18 INFO DAGScheduler: looking for newly runnable stages\n",
      "25/02/15 08:06:18 INFO DAGScheduler: running: Set()\n",
      "25/02/15 08:06:18 INFO DAGScheduler: waiting: Set()\n",
      "25/02/15 08:06:18 INFO DAGScheduler: failed: Set()\n",
      "25/02/15 08:06:18 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "25/02/15 08:06:18 INFO CodeGenerator: Code generated in 64.151289 ms\n",
      "25/02/15 08:06:18 INFO SparkContext: Starting job: toPandas at /home/jovyan/work/recommender-submit.py:37\n",
      "25/02/15 08:06:18 INFO DAGScheduler: Got job 7 (toPandas at /home/jovyan/work/recommender-submit.py:37) with 1 output partitions\n",
      "25/02/15 08:06:18 INFO DAGScheduler: Final stage: ResultStage 12 (toPandas at /home/jovyan/work/recommender-submit.py:37)\n",
      "25/02/15 08:06:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "25/02/15 08:06:18 INFO DAGScheduler: Missing parents: List()\n",
      "25/02/15 08:06:18 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[41] at toPandas at /home/jovyan/work/recommender-submit.py:37), which has no missing parents\n",
      "25/02/15 08:06:18 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 86.9 KiB, free 433.6 MiB)\n",
      "25/02/15 08:06:18 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.2 KiB, free 433.5 MiB)\n",
      "25/02/15 08:06:18 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 36ea6075e503:40667 (size: 36.2 KiB, free: 434.2 MiB)\n",
      "25/02/15 08:06:18 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "25/02/15 08:06:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[41] at toPandas at /home/jovyan/work/recommender-submit.py:37) (first 15 tasks are for partitions Vector(0))\n",
      "25/02/15 08:06:18 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "25/02/15 08:06:18 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 16) (36ea6075e503, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "25/02/15 08:06:18 INFO Executor: Running task 0.0 in stage 12.0 (TID 16)\n",
      "25/02/15 08:06:18 INFO ShuffleBlockFetcherIterator: Getting 1 (11.6 KiB) non-empty blocks including 1 (11.6 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "25/02/15 08:06:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "25/02/15 08:06:18 INFO CodeGenerator: Code generated in 22.06698 ms\n",
      "25/02/15 08:06:18 INFO CodeGenerator: Code generated in 59.284546 ms\n",
      "25/02/15 08:06:18 INFO Executor: Finished task 0.0 in stage 12.0 (TID 16). 16700 bytes result sent to driver\n",
      "25/02/15 08:06:18 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 16) in 188 ms on 36ea6075e503 (executor driver) (1/1)\n",
      "25/02/15 08:06:18 INFO DAGScheduler: ResultStage 12 (toPandas at /home/jovyan/work/recommender-submit.py:37) finished in 0.210 s\n",
      "25/02/15 08:06:18 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "25/02/15 08:06:18 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/02/15 08:06:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "25/02/15 08:06:18 INFO DAGScheduler: Job 7 finished: toPandas at /home/jovyan/work/recommender-submit.py:37, took 0.240287 s\n",
      "25/02/15 08:06:18 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 36ea6075e503:40667 in memory (size: 38.1 KiB, free: 434.2 MiB)\n",
      "closest_users\n",
      " [(596,), (561,), (599,), (380,), (292,), (21,), (586,), (534,), (18,), (305,), (483,), (68,), (573,), (448,), (328,), (352,), (414,), (63,), (514,), (160,), (288,), (91,), (249,), (603,), (62,), (555,), (19,), (220,), (590,), (57,), (45,), (313,), (217,), (330,), (425,), (135,), (202,), (332,), (239,), (520,), (580,), (570,), (489,), (434,), (452,), (182,), (368,), (391,), (354,), (453,), (475,), (469,), (186,), (66,), (608,), (279,), (376,), (597,), (477,), (219,), (198,), (1,), (15,), (304,), (247,), (522,), (166,), (64,), (282,), (428,), (42,), (266,), (607,), (167,), (212,), (129,), (141,), (263,), (577,), (560,), (606,), (551,), (464,), (527,), (432,), (312,), (122,), (201,), (286,), (601,), (195,), (325,), (233,), (474,), (103,), (610,), (525,), (567,), (438,), (144,), (357,), (339,), (125,), (230,), (17,), (381,), (294,), (234,), (200,), (265,), (177,), (226,), (82,), (199,), (552,), (275,), (95,), (562,), (210,), (119,), (274,), (184,), (509,), (318,), (382,), (334,), (298,), (365,), (73,), (256,), (408,), (50,), (306,), (98,), (480,), (246,), (582,), (139,), (372,), (140,), (593,), (301,), (52,), (385,), (466,), (524,), (111,), (137,), (248,), (154,), (393,), (115,), (148,), (600,), (51,), (511,), (28,), (523,), (401,), (252,), (7,), (387,), (331,), (594,), (291,), (30,), (123,), (550,), (106,), (495,), (517,), (105,), (211,), (10,), (227,), (24,), (319,), (344,), (80,), (462,), (153,), (418,), (417,), (222,), (308,), (490,), (564,), (41,), (317,), (326,), (378,), (605,), (492,), (104,), (515,), (556,), (25,), (272,), (114,), (459,), (6,)]\n",
      "25/02/15 08:06:19 INFO SparkContext: Invoking stop() from shutdown hook\n",
      "25/02/15 08:06:19 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "25/02/15 08:06:19 INFO SparkUI: Stopped Spark web UI at http://36ea6075e503:4041\n",
      "25/02/15 08:06:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "25/02/15 08:06:19 INFO MemoryStore: MemoryStore cleared\n",
      "25/02/15 08:06:19 INFO BlockManager: BlockManager stopped\n",
      "25/02/15 08:06:19 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "25/02/15 08:06:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "25/02/15 08:06:19 INFO SparkContext: Successfully stopped SparkContext\n",
      "25/02/15 08:06:19 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/02/15 08:06:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-6a1b0e0f-3c20-4f34-a535-3dc598cbc157\n",
      "25/02/15 08:06:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7c81fae-08ca-4cb4-86e5-030a506ad8bd/pyspark-9b3a56e6-24da-453f-af53-bc4a6bf04678\n",
      "25/02/15 08:06:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7c81fae-08ca-4cb4-86e5-030a506ad8bd\n"
     ]
    }
   ],
   "source": [
    "!spark-submit recommender-submit.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
